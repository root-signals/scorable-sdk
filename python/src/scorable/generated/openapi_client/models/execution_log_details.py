# coding: utf-8

"""
Scorable API

Scorable JSON API provides a way to access Scorable using provisioned API token

The version of the OpenAPI document: 1.0.0 (latest)
Generated by OpenAPI Generator (https://openapi-generator.tech)

Do not edit the class manually.
"""  # noqa: E501

from __future__ import annotations

import json
import pprint
import re  # noqa: F401
from datetime import datetime
from typing import Any, ClassVar, Dict, List, Optional, Set, Union

from pydantic import BaseModel, ConfigDict, StrictFloat, StrictInt, StrictStr
from typing_extensions import Self

from scorable.generated.openapi_client.models.execution_log_details_evaluation_context import (
    ExecutionLogDetailsEvaluationContext,
)
from scorable.generated.openapi_client.models.execution_log_details_evaluator_latencies_inner import (
    ExecutionLogDetailsEvaluatorLatenciesInner,
)
from scorable.generated.openapi_client.models.model_params import ModelParams
from scorable.generated.openapi_client.models.nested_user_details import NestedUserDetails
from scorable.generated.openapi_client.models.skill_execution_validator_result import SkillExecutionValidatorResult


class ExecutionLogDetails(BaseModel):
    """
    ExecutionLogDetails
    """  # noqa: E501

    chat_id: Optional[StrictStr]
    cost: Optional[Union[StrictFloat, StrictInt]]
    created_at: Optional[datetime]
    evaluation_context: ExecutionLogDetailsEvaluationContext
    evaluator_latencies: Optional[List[ExecutionLogDetailsEvaluatorLatenciesInner]]
    executed_item_id: Optional[StrictStr]
    executed_item_name: StrictStr
    executed_item_version_id: Optional[StrictStr]
    execution_type: StrictStr
    id: StrictStr
    justification: StrictStr
    llm_output: StrictStr
    model_call_duration: Union[StrictFloat, StrictInt]
    model_params: Optional[ModelParams] = None
    model: StrictStr
    owner: NestedUserDetails
    parent_execution_log_id: Optional[StrictStr] = None
    prompt_template: StrictStr
    rendered_prompt: StrictStr
    score: Optional[Union[StrictFloat, StrictInt]]
    session_id: StrictStr
    system_prompt: StrictStr
    tags: List[StrictStr]
    user_id: StrictStr
    evaluator_results: List[SkillExecutionValidatorResult]
    variables: Optional[Dict[str, StrictStr]]
    __properties: ClassVar[List[str]] = [
        "chat_id",
        "cost",
        "created_at",
        "evaluation_context",
        "evaluator_latencies",
        "executed_item_id",
        "executed_item_name",
        "executed_item_version_id",
        "execution_type",
        "id",
        "justification",
        "llm_output",
        "model_call_duration",
        "model_params",
        "model",
        "owner",
        "parent_execution_log_id",
        "prompt_template",
        "rendered_prompt",
        "score",
        "session_id",
        "system_prompt",
        "tags",
        "user_id",
        "evaluator_results",
        "variables",
    ]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of ExecutionLogDetails from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        * OpenAPI `readOnly` fields are excluded.
        """
        excluded_fields: Set[str] = set(
            [
                "chat_id",
                "cost",
                "created_at",
                "evaluator_latencies",
                "executed_item_id",
                "executed_item_name",
                "executed_item_version_id",
                "execution_type",
                "id",
                "justification",
                "llm_output",
                "model",
                "owner",
                "prompt_template",
                "rendered_prompt",
                "score",
                "session_id",
                "system_prompt",
                "tags",
                "user_id",
                "evaluator_results",
                "variables",
            ]
        )

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of evaluation_context
        if self.evaluation_context:
            _dict["evaluation_context"] = self.evaluation_context.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in evaluator_latencies (list)
        _items = []
        if self.evaluator_latencies:
            for _item in self.evaluator_latencies:
                if _item:
                    _items.append(_item.to_dict())
            _dict["evaluator_latencies"] = _items
        # override the default output from pydantic by calling `to_dict()` of model_params
        if self.model_params:
            _dict["model_params"] = self.model_params.to_dict()
        # override the default output from pydantic by calling `to_dict()` of owner
        if self.owner:
            _dict["owner"] = self.owner.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in evaluator_results (list)
        _items = []
        if self.evaluator_results:
            for _item in self.evaluator_results:
                if _item:
                    _items.append(_item.to_dict())
            _dict["evaluator_results"] = _items
        # set to None if chat_id (nullable) is None
        # and model_fields_set contains the field
        if self.chat_id is None and "chat_id" in self.model_fields_set:
            _dict["chat_id"] = None

        # set to None if cost (nullable) is None
        # and model_fields_set contains the field
        if self.cost is None and "cost" in self.model_fields_set:
            _dict["cost"] = None

        # set to None if created_at (nullable) is None
        # and model_fields_set contains the field
        if self.created_at is None and "created_at" in self.model_fields_set:
            _dict["created_at"] = None

        # set to None if evaluator_latencies (nullable) is None
        # and model_fields_set contains the field
        if self.evaluator_latencies is None and "evaluator_latencies" in self.model_fields_set:
            _dict["evaluator_latencies"] = None

        # set to None if executed_item_id (nullable) is None
        # and model_fields_set contains the field
        if self.executed_item_id is None and "executed_item_id" in self.model_fields_set:
            _dict["executed_item_id"] = None

        # set to None if executed_item_version_id (nullable) is None
        # and model_fields_set contains the field
        if self.executed_item_version_id is None and "executed_item_version_id" in self.model_fields_set:
            _dict["executed_item_version_id"] = None

        # set to None if model_params (nullable) is None
        # and model_fields_set contains the field
        if self.model_params is None and "model_params" in self.model_fields_set:
            _dict["model_params"] = None

        # set to None if parent_execution_log_id (nullable) is None
        # and model_fields_set contains the field
        if self.parent_execution_log_id is None and "parent_execution_log_id" in self.model_fields_set:
            _dict["parent_execution_log_id"] = None

        # set to None if score (nullable) is None
        # and model_fields_set contains the field
        if self.score is None and "score" in self.model_fields_set:
            _dict["score"] = None

        # set to None if variables (nullable) is None
        # and model_fields_set contains the field
        if self.variables is None and "variables" in self.model_fields_set:
            _dict["variables"] = None

        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of ExecutionLogDetails from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate(
            {
                "chat_id": obj.get("chat_id"),
                "cost": obj.get("cost"),
                "created_at": obj.get("created_at"),
                "evaluation_context": ExecutionLogDetailsEvaluationContext.from_dict(obj["evaluation_context"])
                if obj.get("evaluation_context") is not None
                else None,
                "evaluator_latencies": [
                    ExecutionLogDetailsEvaluatorLatenciesInner.from_dict(_item) for _item in obj["evaluator_latencies"]
                ]
                if obj.get("evaluator_latencies") is not None
                else None,
                "executed_item_id": obj.get("executed_item_id"),
                "executed_item_name": obj.get("executed_item_name"),
                "executed_item_version_id": obj.get("executed_item_version_id"),
                "execution_type": obj.get("execution_type"),
                "id": obj.get("id"),
                "justification": obj.get("justification"),
                "llm_output": obj.get("llm_output"),
                "model_call_duration": obj.get("model_call_duration"),
                "model_params": ModelParams.from_dict(obj["model_params"])
                if obj.get("model_params") is not None
                else None,
                "model": obj.get("model"),
                "owner": NestedUserDetails.from_dict(obj["owner"]) if obj.get("owner") is not None else None,
                "parent_execution_log_id": obj.get("parent_execution_log_id"),
                "prompt_template": obj.get("prompt_template"),
                "rendered_prompt": obj.get("rendered_prompt"),
                "score": obj.get("score"),
                "session_id": obj.get("session_id"),
                "system_prompt": obj.get("system_prompt"),
                "tags": obj.get("tags"),
                "user_id": obj.get("user_id"),
                "evaluator_results": [
                    SkillExecutionValidatorResult.from_dict(_item) for _item in obj["evaluator_results"]
                ]
                if obj.get("evaluator_results") is not None
                else None,
                "variables": obj.get("variables"),
            }
        )
        return _obj
